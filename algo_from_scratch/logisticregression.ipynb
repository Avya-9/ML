{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hypothesis(X, theta):\n",
    "#     \"\"\"\n",
    "#     X - np array (m,n+1)\n",
    "#     theta - np arrary (n+1, 1)\n",
    "#     \"\"\"\n",
    "#     return sigmoid( np.dot(X,theta) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def error(X, y, theta):\n",
    "#     \"\"\"\n",
    "#     params:\n",
    "#         X - np array (m,n+1)\n",
    "#         y - np array (m, 1)\n",
    "#         theta - np arrary (n+1, 1)\n",
    "    \n",
    "#     return :\n",
    "#         scalar value = loss value\n",
    "#     \"\"\"\n",
    "\n",
    "    \n",
    "#     y_hat = hypothesis(X, theta) # (m,1)\n",
    "#     err = (y * np.log(y_hat) + ((1- y) * np.log(1-y_hat)) ).mean()\n",
    "    \n",
    "#     return -err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient(X, y, theta):\n",
    "#     \"\"\"\n",
    "#     X - np array (m,n+1)\n",
    "#     y - np array (m, 1)\n",
    "#     theta - np arrary (n+1, 1)\n",
    "#     \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "#     y_hat = hypothesis(X, theta)\n",
    "#     grad = np.dot( X.T , (y_hat - y) )\n",
    "   \n",
    "#     return grad/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_descent(X, y, lr =0.5, max_itr = 500):\n",
    "#     theta = np.zeros((X.shape[1],1))\n",
    "    \n",
    "#     error_list = []\n",
    "    \n",
    "#     for i in range(max_itr):\n",
    "        \n",
    "#         err = error(X, y, theta)\n",
    "#         error_list.append(err)\n",
    "        \n",
    "#         grad = gradient(X, y, theta)\n",
    "        \n",
    "#         # Updation Rule\n",
    "#         theta = theta - lr*grad\n",
    "        \n",
    "#     return (theta, error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self,lr=0.1,n_iters=1000,regularization=None,reg_strength=0.01,random_state=None):\n",
    "        self.lr=lr\n",
    "        self.n_iters=n_iters\n",
    "        self.weights=None\n",
    "        self.bias=None\n",
    "        self.scaler=StandardScaler()\n",
    "        self.regularization=regularization\n",
    "        self.reg_strength=reg_strength\n",
    "        self.random_state=random_state\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            X, y = shuffle(X, y, random_state=self.random_state)\n",
    "        # scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        n_samples,n_features=X_scaled.shape\n",
    "        # self.weights=np.zeros(n_features)\n",
    "        self.weights=np.random.rand(n_features)*0.01\n",
    "        self.bias=0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred=np.dot(X_scaled,self.weights)+self.bias\n",
    "            predictions=sigmoid(linear_pred)\n",
    "\n",
    "            dw=(1/n_samples)* np.dot(X.T,(predictions-y))\n",
    "            db=(1/n_samples)* np.sum(predictions-y)\n",
    "            if self.regularization == 'l2':\n",
    "                dw += (self.reg_strength / n_samples) * self.weights\n",
    "            elif self.regularization == 'l1':\n",
    "                dw += (self.reg_strength / n_samples) * np.sign(self.weights)\n",
    "            self.weights=self.weights-self.lr*dw\n",
    "            self.bias=self.bias-self.lr*db\n",
    "\n",
    "    def predict(self,X):\n",
    "        # scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        linear_pred=np.dot(X,self.weights)+self.bias\n",
    "        y_pred=sigmoid(linear_pred)\n",
    "        class_pred=[0 if y<=0.5 else 1 for y in y_pred]\n",
    "        return class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  Q  S  male\n",
       "0         0       3  22.0      1      0   7.2500  0  1     1\n",
       "1         1       3  26.0      0      0   7.9250  0  1     0\n",
       "2         1       1  35.0      1      0  53.1000  0  1     0\n",
       "3         0       3  35.0      0      0   8.0500  0  1     1\n",
       "4         0       1  54.0      0      0  51.8625  0  1     1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/avyaahuja/XLSCOUT/titanic/refinedtitanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[['Age', 'Fare', 'male', 'Pclass']], df['Survived'], test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('/Users/avyaahuja/XLSCOUT/algo_from_scratch/clean_anaemia.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies=pd.get_dummies(df['Anaemic'],drop_first=True)\n",
    "# dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies[\"Yes\"] = dummies[\"Yes\"].astype(int)\n",
    "# dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Anaemic']=dummies['Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# x=df.drop('Anaemic',axis='columns')\n",
    "# y=df['Anaemic']\n",
    "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6721311475409836"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=LogisticRegression(lr=0.01,n_iters=400,regularization='l1',reg_strength=0.9,random_state=29)\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test.values)\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6309278350515464"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=classifier.predict(x_train.values)\n",
    "acc=accuracy_score(y_train,y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avyaahuja/XLSCOUT/.venv/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8114754098360656"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier=LogisticRegression()\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test.values)\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avyaahuja/XLSCOUT/.venv/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8041237113402062"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=classifier.predict(x_train.values)\n",
    "acc=accuracy_score(y_train,y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# class LogisticRegression:\n",
    "#     def __init__(self, lr=0.001, n_iters=1000, regularization='none', alpha=0.01):\n",
    "#         self.lr = lr\n",
    "#         self.n_iters = n_iters\n",
    "#         self.weights = None\n",
    "#         self.bias = None\n",
    "#         self.regularization = regularization\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         self.weights = np.zeros(n_features)\n",
    "#         self.bias = 0\n",
    "\n",
    "#         for _ in range(self.n_iters):\n",
    "#             linear_pred = np.dot(X, self.weights) + self.bias\n",
    "#             predictions = sigmoid(linear_pred)\n",
    "\n",
    "#             dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n",
    "#             db = (1 / n_samples) * np.sum(predictions - y)\n",
    "\n",
    "#             if self.regularization == 'l2':\n",
    "#                 dw += (self.alpha / n_samples) * self.weights\n",
    "#             elif self.regularization == 'l1':\n",
    "#                 dw += (self.alpha / n_samples) * np.sign(self.weights)\n",
    "\n",
    "#             self.weights -= self.lr * dw\n",
    "#             self.bias -= self.lr * db\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         linear_pred = np.dot(X, self.weights) + self.bias\n",
    "#         y_pred = sigmoid(linear_pred)\n",
    "#         class_pred = [0 if y <= 0.5 else 1 for y in y_pred]\n",
    "#         return class_pred\n",
    "\n",
    "#     def accuracy(self, y_true, y_pred):\n",
    "#         return np.sum(y_true == y_pred)/len(y_true)\n",
    "\n",
    "# def custom_grid_search(X, y, param_grid, cv=5, random_state_range=None):\n",
    "#     best_params = None\n",
    "#     best_score = 0\n",
    "#     best_random_state = None\n",
    "\n",
    "#     if random_state_range is None:\n",
    "#         random_state_range = [None]\n",
    "\n",
    "#     for random_state in random_state_range:\n",
    "#         for lr in param_grid['lr']:\n",
    "#             for n_iters in param_grid['n_iters']:\n",
    "#                 for regularization in param_grid['regularization']:\n",
    "#                     for alpha in param_grid['alpha']:\n",
    "#                         scores = []\n",
    "#                         kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "#                         for train_index, val_index in kf.split(X):\n",
    "#                             X_train, X_val = X[train_index], X[val_index]\n",
    "#                             y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "#                             model = LogisticRegression(lr=lr, n_iters=n_iters, regularization=regularization, alpha=alpha)\n",
    "#                             model.fit(X_train, y_train)\n",
    "#                             y_pred = model.predict(X_val)\n",
    "#                             scores.append(model.accuracy(y_val, y_pred))\n",
    "\n",
    "#                         mean_score = np.mean(scores)\n",
    "#                         if mean_score > best_score:\n",
    "#                             best_score = mean_score\n",
    "#                             best_params = {'lr': lr, 'n_iters': n_iters, 'regularization': regularization, 'alpha': alpha}\n",
    "#                             best_random_state = random_state\n",
    "\n",
    "#     return best_params, best_score, best_random_state\n",
    "\n",
    "# # Create a sample dataset\n",
    "# X=df[['Age','Fare','male','Pclass']]\n",
    "# y=df['Survived']\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# # Ensure X and y are numpy arrays\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# # Split the dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'lr': [0.001, 0.01, 0.1],\n",
    "#     'n_iters': [1000, 2000],\n",
    "#     'regularization': ['none', 'l1', 'l2'],\n",
    "#     'alpha': [0.01, 0.1, 1.0]\n",
    "# }\n",
    "\n",
    "# # Define a range of random states to search over\n",
    "# random_state_range = list(range(10, 50))\n",
    "\n",
    "# # Perform custom grid search\n",
    "# best_params, best_score, best_random_state = custom_grid_search(X_train, y_train, param_grid, cv=5, random_state_range=random_state_range)\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "# print(f\"Best cross-validation accuracy: {best_score}\")\n",
    "# print(f\"Best random state: {best_random_state}\")\n",
    "\n",
    "# # Train and evaluate the best model on the test set with the best random state\n",
    "# best_model = LogisticRegression(**best_params)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=best_random_state)\n",
    "# best_model.fit(X_train, y_train)\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# test_accuracy = best_model.accuracy(y_test, y_pred)\n",
    "# print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
